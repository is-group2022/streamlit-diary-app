import streamlit as st
import pandas as pd
import gspread
import datetime
import urllib.parse
import re
from google.cloud import storage

# --- 1. å®šæ•°ãƒ»è¨­å®š ---
try:
    SHEET_ID = st.secrets["google_resources"]["spreadsheet_id"]
    GCS_BUCKET_NAME = "auto-poster-images"
    ACCOUNT_OPTIONS = ["A", "B", "C", "D"]
    SHEET_MAP = {opt: f"æŠ•ç¨¿{opt}ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ" for opt in ACCOUNT_OPTIONS}
    DF_COLS = ["ã‚¨ãƒªã‚¢", "åº—å", "åª’ä½“", "æŠ•ç¨¿æ™‚é–“", "å¥³ã®å­ã®åå‰", "ã‚¿ã‚¤ãƒˆãƒ«", "æœ¬æ–‡"]
except KeyError:
    st.error("ğŸš¨ secrets.tomlã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- 2. è£œåŠ©é–¢æ•° ---
def normalize_text(s):
    if not s: return ""
    return re.sub(r'\s+', '', str(s)).replace('ã€€', '').lower()

def parse_to_datetime(t_str):
    t_clean = re.sub(r'[^0-9]', '', str(t_str))
    if len(t_clean) == 3: t_clean = "0" + t_clean
    if len(t_clean) == 4:
        try: return datetime.datetime.strptime(t_clean, "%H%M")
        except: return None
    return None

def is_time_match(base_time, target_filename, window_min=20):
    if not base_time: return False
    match = re.match(r'^(\d{3,4})', target_filename)
    if not match: return False
    t_target = parse_to_datetime(match.group(1))
    if not t_target: return False
    diff = abs((base_time - t_target).total_seconds()) / 60
    return diff <= window_min or diff >= (1440 - window_min)

def get_cached_url(blob_name):
    return f"https://storage.googleapis.com/{GCS_BUCKET_NAME}/{urllib.parse.quote(blob_name)}"

# --- 3. APIæ¥ç¶š ---
@st.cache_resource(ttl=3600)
def get_clients():
    gc = gspread.service_account_from_dict(st.secrets["gcp_service_account"])
    gcs = storage.Client.from_service_account_info(st.secrets["gcp_service_account"])
    return gc, gcs

GC, GCS_CLIENT = get_clients()
SPRS = GC.open_by_key(SHEET_ID)

# --- 4. UIæ§‹ç¯‰ ---
# initial_sidebar_state="expanded" ã§ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’æœ€åˆã‹ã‚‰é–‹ãè¨­å®šã«
st.set_page_config(
    layout="wide", 
    page_title="å†™ãƒ¡æ—¥è¨˜ã‚¨ãƒ‡ã‚£ã‚¿",
    initial_sidebar_state="expanded" 
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
    <style>
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¶ˆã™ */
    [data-testid="stHeader"] { display: none; }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼è‡ªä½“ã®èƒŒæ™¯è‰²ã‚’å°‘ã—å¤‰ãˆã¦å­˜åœ¨æ„Ÿã‚’å‡ºã™ */
    [data-testid="stSidebar"] {
        background-color: #f8f9fa;
        border-right: 1px solid #e0e0e0;
    }
    
    /* æœ¬æ–‡ã®å…¥åŠ›æ¬„ */
    .stTextArea textarea { font-size: 14px; line-height: 1.5; }
    
    /* ã‚«ãƒ¼ãƒ‰ã®åŒºåˆ‡ã‚Š */
    .diary-entry {
        border-bottom: 2px solid #eee;
        padding-bottom: 20px;
        margin-bottom: 20px;
    }
    </style>
""", unsafe_allow_html=True)

def main():
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã™ã¹ã¦ã®é¸æŠè¦ç´ ã‚’é…ç½®
    with st.sidebar:
        st.title("âš™ï¸ é¸æŠãƒ‘ãƒãƒ«")
        st.info("ã“ã“ã§æ¡ä»¶ã‚’é¸ã‚“ã§ãã ã•ã„")
        
        # 1. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé¸æŠ
        sel_acc = st.selectbox("ğŸ‘¤ æŠ•ç¨¿ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", ACCOUNT_OPTIONS, index=0)
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        ws = SPRS.worksheet(SHEET_MAP[sel_acc])
        data = ws.get_all_values()
        
        if len(data) > 1:
            full_df = pd.DataFrame(data[1:])
            full_df = full_df.iloc[:, :7]
            while full_df.shape[1] < 7: full_df[full_df.shape[1]] = ""
            full_df.columns = DF_COLS
            full_df['__row__'] = range(2, len(data) + 1)

            # 2. ã‚¨ãƒªã‚¢é¸æŠ
            areas = sorted(full_df["ã‚¨ãƒªã‚¢"].unique())
            sel_area = st.selectbox("ğŸ“ ã‚¨ãƒªã‚¢", ["æœªé¸æŠ"] + areas)
            
            sel_store = "æœªé¸æŠ"
            if sel_area != "æœªé¸æŠ":
                # 3. åº—èˆ—é¸æŠ
                stores = sorted(full_df[full_df["ã‚¨ãƒªã‚¢"] == sel_area]["åº—å"].unique())
                sel_store = st.selectbox("ğŸ¢ åº—èˆ—", ["æœªé¸æŠ"] + stores)
        else:
            st.error("ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

    # ãƒ¡ã‚¤ãƒ³ç”»é¢ã®è¡¨ç¤ºåˆ¶å¾¡
    if sel_store == "æœªé¸æŠ":
        st.title("ğŸ“¸ å†™ãƒ¡æ—¥è¨˜ã‚¨ãƒ‡ã‚£ã‚¿ Pro")
        st.warning("ğŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œã‚¨ãƒªã‚¢ã€ã¨ã€Œåº—èˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.caption("â€»ã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒé–‰ã¾ã£ã¦ã„ã‚‹å ´åˆã¯ã€å·¦ä¸Šã®ã€Œ > ã€ãƒãƒ¼ã‚¯ã‚’æŠ¼ã—ã¦é–‹ã„ã¦ãã ã•ã„ã€‚")
        return

    # --- ä»¥ä¸‹ã€é¸æŠå¾Œã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
    st.title(f"ğŸ¢ {sel_store} ã®ç·¨é›†")
    
    target_df = full_df[(full_df["ã‚¨ãƒªã‚¢"] == sel_area) & (full_df["åº—å"] == sel_store)]
    
    # çµ±è¨ˆ
    m_c1, m_c2 = st.columns([1, 3])
    m_c1.metric("åˆè¨ˆä»¶æ•°", f"{len(target_df)} ä»¶")

    # GCSç”»åƒå–å¾—
    bucket = GCS_CLIENT.bucket(GCS_BUCKET_NAME)
    blobs = list(bucket.list_blobs(prefix=f"{sel_area}/"))
    store_norm = normalize_text(sel_store)
    store_images = [b.name for b in blobs if normalize_text(b.name.split('/')[1]) in [store_norm, normalize_text(f"ãƒ‡ãƒªã˜ã‚ƒ{sel_store}")]]

    st.write("---")

    for idx, row in target_df.iterrows():
        base_time = parse_to_datetime(row["æŠ•ç¨¿æ™‚é–“"])
        name_norm = normalize_text(row["å¥³ã®å­ã®åå‰"])
        matched_files = [img for img in store_images if name_norm in normalize_text(img.split('/')[-1]) and is_time_match(base_time, img.split('/')[-1])]

        with st.container():
            st.markdown(f"#### ğŸ‘¤ {row['å¥³ã®å­ã®åå‰']} / â° {row['æŠ•ç¨¿æ™‚é–“']}")
            col_txt, col_img, col_ops = st.columns([2.5, 1, 1])

            with col_txt:
                new_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«", row["ã‚¿ã‚¤ãƒˆãƒ«"], key=f"ti_{idx}")
                new_body = st.text_area("æœ¬æ–‡", row["æœ¬æ–‡"], key=f"bo_{idx}", height=300) # æœ¬æ–‡ã‚’ã•ã‚‰ã«è¦‹ã‚„ã™ã
                
                if st.button("ğŸ’¾ ã“ã®å†…å®¹ã‚’ä¿å­˜", key=f"sv_{idx}", type="primary"):
                    ws.update_cell(row['__row__'], 6, new_title)
                    ws.update_cell(row['__row__'], 7, new_body)
                    st.toast("ä¿å­˜ã—ã¾ã—ãŸï¼")

            with col_img:
                if matched_files:
                    for m_path in matched_files:
                        st.image(get_cached_url(m_path), use_container_width=True)
                        with st.popover("ğŸ—‘ï¸ å‰Šé™¤"):
                            st.write("æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
                            if st.button("å®Ÿè¡Œ", key=f"del_{idx}_{m_path}"):
                                bucket.blob(m_path).delete()
                                st.cache_data.clear()
                                st.rerun()
                else:
                    st.error("ğŸš¨ ç”»åƒãªã—")

            with col_ops:
                up_file = st.file_uploader("ğŸ“¥ ç”»åƒè¿½åŠ ", type=["jpg","png","jpeg"], key=f"up_{idx}")
                if up_file:
                    if st.button("ğŸš€ ã‚¢ãƒƒãƒ—", key=f"u_btn_{idx}"):
                        ext = up_file.name.split('.')[-1]
                        folder_name = f"ãƒ‡ãƒªã˜ã‚ƒ {sel_store}" if row["åª’ä½“"] == "ãƒ‡ãƒªã˜ã‚ƒ" else sel_store
                        new_blob_name = f"{sel_area}/{folder_name}/{row['æŠ•ç¨¿æ™‚é–“']}_{row['å¥³ã®å­ã®åå‰']}.{ext}"
                        blob = bucket.blob(new_blob_name)
                        blob.upload_from_string(up_file.getvalue(), content_type=up_file.type)
                        st.cache_data.clear()
                        st.rerun()
            
            st.markdown("<div class='diary-entry'></div>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
