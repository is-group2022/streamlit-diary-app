import streamlit as st
import pandas as pd
import gspread
import datetime
import urllib.parse
import re
from google.cloud import storage

# --- 1. å®šæ•°ãƒ»è¨­å®š ---
try:
    SHEET_ID = st.secrets["google_resources"]["spreadsheet_id"]
    GCS_BUCKET_NAME = "auto-poster-images"
    ACCOUNT_OPTIONS = ["A", "B", "C", "D"]
    SHEET_MAP = {opt: f"æŠ•ç¨¿{opt}ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ" for opt in ACCOUNT_OPTIONS}
    DF_COLS = ["ã‚¨ãƒªã‚¢", "åº—å", "åª’ä½“", "æŠ•ç¨¿æ™‚é–“", "å¥³ã®å­ã®åå‰", "ã‚¿ã‚¤ãƒˆãƒ«", "æœ¬æ–‡"]
except KeyError:
    st.error("ğŸš¨ secrets.tomlã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- 2. è£œåŠ©é–¢æ•° ---
def normalize_text(s):
    if not s: return ""
    return re.sub(r'\s+', '', str(s)).replace('ã€€', '').lower()

def parse_to_datetime(t_str):
    t_clean = re.sub(r'[^0-9]', '', str(t_str))
    if len(t_clean) == 3: t_clean = "0" + t_clean
    if len(t_clean) == 4:
        try: return datetime.datetime.strptime(t_clean, "%H%M")
        except: return None
    return None

def is_time_match(base_time, target_filename, window_min=20):
    if not base_time: return False
    match = re.match(r'^(\d{3,4})', target_filename)
    if not match: return False
    t_target = parse_to_datetime(match.group(1))
    if not t_target: return False
    diff = abs((base_time - t_target).total_seconds()) / 60
    return diff <= window_min or diff >= (1440 - window_min)

def get_cached_url(blob_name):
    return f"https://storage.googleapis.com/{GCS_BUCKET_NAME}/{urllib.parse.quote(blob_name)}"

# --- 3. APIæ¥ç¶š ---
@st.cache_resource(ttl=3600)
def get_clients():
    gc = gspread.service_account_from_dict(st.secrets["gcp_service_account"])
    gcs = storage.Client.from_service_account_info(st.secrets["gcp_service_account"])
    return gc, gcs

GC, GCS_CLIENT = get_clients()
SPRS = GC.open_by_key(SHEET_ID)

# --- 4. UIæ§‹ç¯‰ ---
st.set_page_config(layout="wide", page_title="å†™ãƒ¡æ—¥è¨˜ã‚¨ãƒ‡ã‚£ã‚¿")

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
    <style>
    [data-testid="stHeader"] { display: none; }
    /* é¸æŠãƒ‘ãƒãƒ«ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .filter-panel {
        background-color: #f1f3f6;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 25px;
        border: 1px solid #d1d5db;
    }
    .stTextArea textarea { font-size: 15px; line-height: 1.6; }
    /* åŒºåˆ‡ã‚Šç·š */
    .diary-divider {
        border-bottom: 2px solid #eee;
        padding-bottom: 30px;
        margin-bottom: 30px;
    }
    </style>
""", unsafe_allow_html=True)

def main():
    st.title("ğŸ“¸ å†™ãƒ¡æ—¥è¨˜æŠ•ç¨¿ç®¡ç†")

    # --- ãƒ¡ã‚¤ãƒ³ç”»é¢ä¸Šéƒ¨ã®é¸æŠãƒ‘ãƒãƒ« (å¸¸ã«è¡¨ç¤º) ---
    st.markdown('<div class="filter-panel">', unsafe_allow_html=True)
    c1, c2, c3 = st.columns(3)
    
    with c1:
        sel_acc = st.selectbox("ğŸ‘¤ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", ACCOUNT_OPTIONS, index=0)
    
    # é¸æŠã•ã‚ŒãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    ws = SPRS.worksheet(SHEET_MAP[sel_acc])
    data = ws.get_all_values()
    
    if len(data) <= 1:
        st.warning("ã“ã®ã‚·ãƒ¼ãƒˆã«ã¯æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.markdown('</div>', unsafe_allow_html=True)
        return
        
    full_df = pd.DataFrame(data[1:])
    full_df = full_df.iloc[:, :7]
    while full_df.shape[1] < 7: full_df[full_df.shape[1]] = ""
    full_df.columns = DF_COLS
    full_df['__row__'] = range(2, len(data) + 1)

    with c2:
        areas = sorted(full_df["ã‚¨ãƒªã‚¢"].unique())
        sel_area = st.selectbox("ğŸ“ ã‚¨ãƒªã‚¢", ["æœªé¸æŠ"] + areas)
    
    sel_store = "æœªé¸æŠ"
    with c3:
        if sel_area != "æœªé¸æŠ":
            stores = sorted(full_df[full_df["ã‚¨ãƒªã‚¢"] == sel_area]["åº—å"].unique())
            sel_store = st.selectbox("ğŸ¢ åº—èˆ—", ["æœªé¸æŠ"] + stores)
        else:
            st.selectbox("ğŸ¢ åº—èˆ—", ["ã‚¨ãƒªã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„"], disabled=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

    # æœªé¸æŠæ™‚ã®ã‚¬ã‚¤ãƒ‰
    if sel_store == "æœªé¸æŠ":
        st.info("ğŸ’¡ ä¸Šè¨˜ã®ãƒ‘ãƒãƒ«ã‹ã‚‰ã€Œã‚¨ãƒªã‚¢ã€ã¨ã€Œåº—èˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return

    # --- é¸æŠå¾Œã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
    target_df = full_df[(full_df["ã‚¨ãƒªã‚¢"] == sel_area) & (full_df["åº—å"] == sel_store)]
    
    st.subheader(f"ğŸ“Š {sel_store} (åˆè¨ˆ: {len(target_df)} ä»¶)")

    # GCSç”»åƒå–å¾—
    bucket = GCS_CLIENT.bucket(GCS_BUCKET_NAME)
    blobs = list(bucket.list_blobs(prefix=f"{sel_area}/"))
    store_norm = normalize_text(sel_store)
    store_images = [b.name for b in blobs if normalize_text(b.name.split('/')[1]) in [store_norm, normalize_text(f"ãƒ‡ãƒªã˜ã‚ƒ{sel_store}")]]

    st.write("---")

    for idx, row in target_df.iterrows():
        base_time = parse_to_datetime(row["æŠ•ç¨¿æ™‚é–“"])
        name_norm = normalize_text(row["å¥³ã®å­ã®åå‰"])
        matched_files = [img for img in store_images if name_norm in normalize_text(img.split('/')[-1]) and is_time_match(base_time, img.split('/')[-1])]

        with st.container():
            st.markdown(f"#### ğŸ‘¤ {row['å¥³ã®å­ã®åå‰']} / â° {row['æŠ•ç¨¿æ™‚é–“']}")
            col_txt, col_img, col_ops = st.columns([2.5, 1, 1])

            with col_txt:
                new_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«", row["ã‚¿ã‚¤ãƒˆãƒ«"], key=f"ti_{idx}")
                # æœ¬æ–‡ã‚’ã•ã‚‰ã«å¤§ããè¡¨ç¤º (height=400)
                new_body = st.text_area("æœ¬æ–‡", row["æœ¬æ–‡"], key=f"bo_{idx}", height=400)
                
                if st.button("ğŸ’¾ å†…å®¹ã‚’ä¿å­˜", key=f"sv_{idx}", type="primary"):
                    ws.update_cell(row['__row__'], 6, new_title)
                    ws.update_cell(row['__row__'], 7, new_body)
                    st.toast("ä¿å­˜ã—ã¾ã—ãŸï¼")

            with col_img:
                if matched_files:
                    for m_path in matched_files:
                        st.image(get_cached_url(m_path), use_container_width=True)
                        with st.popover("ğŸ—‘ï¸ å‰Šé™¤"):
                            st.write("æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
                            if st.button("å®Ÿè¡Œã™ã‚‹", key=f"del_{idx}_{m_path}"):
                                bucket.blob(m_path).delete()
                                st.cache_data.clear()
                                st.rerun()
                else:
                    st.error("ğŸš¨ ç”»åƒãªã—")

            with col_ops:
                up_file = st.file_uploader("ğŸ“¥ ç”»åƒè¿½åŠ ", type=["jpg","png","jpeg"], key=f"up_{idx}")
                if up_file:
                    if st.button("ğŸš€ ã‚¢ãƒƒãƒ—", key=f"u_btn_{idx}"):
                        ext = up_file.name.split('.')[-1]
                        folder_name = f"ãƒ‡ãƒªã˜ã‚ƒ {sel_store}" if row["åª’ä½“"] == "ãƒ‡ãƒªã˜ã‚ƒ" else sel_store
                        new_blob_name = f"{sel_area}/{folder_name}/{row['æŠ•ç¨¿æ™‚é–“']}_{row['å¥³ã®å­ã®åå‰']}.{ext}"
                        blob = bucket.blob(new_blob_name)
                        blob.upload_from_string(up_file.getvalue(), content_type=up_file.type)
                        st.cache_data.clear()
                        st.rerun()
            
            # åŒºåˆ‡ã‚Šç”¨HTML (SyntaxErrorã‚’ä¿®æ­£)
            st.markdown("<div class='diary-divider'></div>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()

